Этап 1. Анализ требований и архитектуры  
Статус: выполнено  
Задачи:  
- Подробно разобрать документацию из `docs/requirements.md` и существующую архитектуру.  
- Зафиксировать технологический стек, ограничения по API и дедупликации.  
- Определить целевые метрики пайплайна (объём компаний, лимиты писем, окна запуска).  
- Подготовить черновик диаграммы компонентов для `docs/architecture.md`.

Этап 2. Проектирование Docker-инфраструктуры  
Статус: выполнено  
Задачи:  
- Спроектировать структуру `docker-compose.yml` с сервисами: `app`, `worker/scheduler`, `db`, `redis` (если потребуется очереди/кэш), вспомогательные утилиты.  
- Выбрать базовый образ (Python 3.11+) и продумать multi-stage сборку.  
- Описать конфигурацию сетей, хранилища данных, переменные окружения и секреты.  
- Обновить `.env.example` и `.env` (с комментариями по источникам доступа).  
- Обновить `README.md` с инструкциями по развёртыванию в Docker.

Этап 3. Проектирование базы данных и миграций  
Статус: выполнено  
Задачи:  
- Определить структуру таблиц для сущностей: компании, домены, контакты, письма, SERP-запросы, очереди задач.  
- Подготовить миграции (SQL/алгоритмические) и описать правила идемпотентности.  
- Настроить обёртку для подключения (например, SQLAlchemy) и вынести параметры в `config.py`.  
- Подготовить тестовые фикстуры/seed-данные для `tests`.

Этап 4. Реализация модуля Yandex Deferred Search  
Статус: выполнено  
Задачи:  
- Реализовать `modules/yandex_deferred.py` для создания deferred-запросов и опроса статусов.  
- Настроить ретраи, ограничение по ночным окнам, учёт квот.  
- Интегрировать модуль с планировщиком задач и логированием.  
- Добавить unit/интеграционные тесты для API-клиента (моки ответов).

Этап 5. Обработка SERP и нормализация  
Статус: выполнено  
Задачи:  
- Реализовать `modules/serp_ingest.py` (парсинг XML, сохранение результатов, нормализация URL/доменов).  
- Связать результаты с таблицами `serp_results` и `companies`.  
- Обработать ошибки парсинга и пустые ответы.  
- Написать тесты на нормализацию и сохранение данных.

Этап 6. Дедупликация и управление сущностями  
Статус: выполнено  
Задачи:  
- Реализовать `modules/deduplicate.py` с чёткими и «фаззи» правилами.  
- Настроить унификацию названий компаний, каноникализацию доменов, учёт Opt-out.  
- Обновить архитектурную документацию (`docs/architecture.md`) описанием потоков дедупликации.  
- Покрыть юнит-тестами кейсы дедупликации.

Этап 7. Обогащение контактов  
Статус: выполнено  
Задачи:  
- Реализовать `modules/enrich_contacts.py`: загрузка страниц, поиск контактов, фильтры e-mail/телефонов.  
- Настроить очереди/воркеры (в Docker) для параллельной обработки, учесть rate-limits.  
- Добавить обработку ошибок, ретраи, кеширование.  
- Покрыть тестами извлечение контактов и обработку исключений.

Этап 8. Генерация и отправка писем  
Статус: выполнено  
Задачи:  
- Реализовать `modules/generate_email_gpt.py` (интеграция с LLM, шаблоны выгод, персонализация).  
- Реализовать `modules/send_email.py` с поддержкой Gmail SMTP и возможностью расширения до Mailgun/Brevo.  
- Настроить хранение статусов отправки, обработку отказов, учёт Opt-out.  
- Обновить тесты для генерации текстов (моки) и отправки.

Этап 9. Оркестрация и планировщик  
Статус: выполнено  
Задачи:  
- Реализовать `main.py` и планировщик ночных задач (например, APScheduler/Celery Beat).  
- Связать между собой все модули, обеспечить идемпотентность шагов.  
- Настроить структурированное логирование, алерты, health-check endpoints.  
- Добавить end-to-end тесты пайплайна (можно с mock API).

Этап 10. Финальное тестирование и документация  
Статус: выполнено  
Задачи:  
- Запустить полный пайплайн в Docker, подтвердить соответствие метрикам и требованиям.  
- Обновить `docs/architecture.md` финальной архитектурной схемой и описанием потоков данных.  
- Дополнить `README.md` инструкциями по развёртыванию на удалённом сервере.  
- Зафиксировать результаты тестов и подготовить чек-лист контроля качества.

Этап 11. Автоматизация получения IAM токена  
Статус: выполнено  
Задачи:  
- Реализовать провайдер IAM токенов на основе ключа сервисного аккаунта.  
- Интегрировать провайдер в `YandexDeferredClient` и оркестратор.  
- Обновить `.env.example`, `README.md`, архитектурную документацию и тесты.

Этап 12. Интеграция вводной витрины (ниш/городов)  
Статус: выполнено  
Задачи:  
- Подключить Google Sheets API к листу `NICHES_INPUT`, читать поля `niche`, `city`, `country`, `batch_tag`.  
- Реализовать сервис подготовки партии: валидация обязательных полей, обогащение кодами региона (`lr`) по словарю, расчёт языка/триггеров, формирование `metadata`.  
- Обновлять служебные столбцы (`status`, `generated_count`, `db_inserted_count`, `db_duplicate_count`, `db_first_scheduled_for`, `db_last_scheduled_for`, `last_error`) после каждой синхронизации.  
- Добавить ручной CLI/оркестраторный шаг для запуска генерации по выбранной партии (`batch_tag`).

Этап 13. Генерация поисковых запросов и постановка в очередь  
Статус: выполнено  
Задачи:  
- Реализовать генератор `query_text`: префикс `lang:ru`, ниша, опциональный город/страна, триггеры и минус-домены из конфига `DEFAULTS_min_no_files`.  
- Ограничивать число запросов на нишу (≤6) и равномерно распределять `scheduled_for` внутри ближайшего ночного окна (`20:00-05:59` UTC) с шагом `spacing_seconds`.  
- Вычислять `query_hash = SHA1(query_text|region_code)` и записывать записи в `serp_queries` со статусом `pending`, `is_night_window = TRUE`.  
- Логировать результат (сколько добавлено/дубликатов) и сохранять параметры партии в `metadata` (`niche`, `city`, `country`, `trigger`, `batch_tag`, `language`, `selection`).

Этап 14. Хранилище результатов и отчётность  
Статус: выполнено  
Задачи:  
- Расширить БД дополнительными представлениями/таблицами из `requirements-for-filling-out-the-database.md` (`daily_summaries`, `top_domains`, `company_status_view`).  
- Настроить обновление `companies` по результатам нормализации (главная страница, источник `yandex_search_api`, таймстемпы).  
- Реализовать выгрузку рабочих доменов для downstream-процессов (CSV/таблица) и интегрировать проверки качества (дедуп, количество минус-доменов).  
- Добавить SQL/CLI-скрипты для аналитики (`SELECT` из `daily_summaries` и др.) и описать их в документации.
